parent_config: './conf/qa/squad/abstract_squad.yaml'
seed: 1337

repr_dim: 128
dropout: 0.2
learning_rate: 0.0005
learning_rate_decay: 0.8
with_wiq: False
num_interactive: 0
epochs: 15
is_interactive: False

reader: 'modular_qa_reader'
name: 'qanet'


model:
  encoder_layer:
    ################ QUESTION ##################
    - input: ['question', 'char_question']
      output: 'question'
      module: 'concat'
    - input: 'question'
      name: 'embedding_highway'  # use same network as support
      module: 'highway'
    - input: 'question'
      output: 'emb_question'
      name: 'embedding_projection'  # use same network as support
      module: 'dense'
      activation: 'tanh'
      dropout: True

    - input: 'question'
      module: 'positional_encoding'
      name: 'postional_enc'

    - input: 'question'
      module: 'conv_separable'
      conv_width: 7
      num_layers: 4
      residual: True
      layer_norm: True
      name: 'convnet'

    - input: 'question'
      output: 'question_self'
      module: 'dense'
      repr_dim: 16
      activation: 'tanh'
      layer_norm: True
      name: 'self_attn_prepare'

    - input: 'question_self'
      module: 'self_attn'
      attn_type: 'dot'
      repr_dim: 16
      scaled: True
      num_attn_heads: 8
      key_value_attn: True
      layer_norm: True
      name: 'self_attn'

    - input: 'question_self'
      module: 'dense'
      activation: 'relu'
      name: 'self_attn_projection'

    - input: ['question', 'question_self']
      output: 'question'
      module: 'add'

    - input: 'question'
      module: 'dense'
      activation: 'tanh'
      name: 'final_projection'
      layer_norm: True

    ################ SUPPORT ##################
    - input: ['support', 'char_support']
      output: 'support'
      module: 'concat'
    - input: 'support'
      name: 'embedding_highway'  # use same network as support
      module: 'highway'
    - input: 'support'
      output: 'emb_support'
      name: 'embedding_projection'  # use same network as support
      module: 'dense'
      activation: 'tanh'
      dropout: True

    - input: 'support'
      module: 'positional_encoding'
      name: 'postional_enc'

    - input: 'support'
      module: 'conv_separable'
      conv_width: 7
      num_layers: 4
      residual: True
      layer_norm: True
      name: 'convnet'

    - input: 'support'
      output: 'support_self'
      module: 'dense'
      repr_dim: 16
      activation: 'tanh'
      layer_norm: True
      name: 'self_attn_prepare'

    - input: 'support_self'
      module: 'self_attn'
      attn_type: 'dot'
      repr_dim: 16
      scaled: True
      num_attn_heads: 8
      key_value_attn: True
      layer_norm: True
      name: 'self_attn'

    - input: 'support_self'
      module: 'dense'
      activation: 'relu'
      name: 'self_attn_projection'

    - input: ['support', 'support_self']
      output: 'support'
      module: 'add'

    - input: 'support'
      module: 'dense'
      activation: 'tanh'
      name: 'final_projection'
      layer_norm: True

    ################ MODELING ##################
    - input: 'support'
      dependent: 'question'
      output: 'support_attn'
      module: 'coattention'
      attn_type: 'diagonal_bilinear'
      concat: False

    - input: ['support', 'support']
      output: 'support_x2'
      module: 'concat'

    - input: ['support_x2', 'support_attn']
      output: 'support_attn_mul'
      module: 'mul'

    - input: ['support', 'support_attn', 'support_attn_mul']
      output: 'support'
      module: 'concat'

    - input: 'support'
      module: 'dense'
      activation: 'tanh'

    - module: 'repeat'
      num: 7
      reuse: False
      encoder:
        - input: 'support'
          module: 'positional_encoding'

        - input: 'support'
          module: 'conv_separable'
          conv_width: 7
          num_layers: 2
          residual: True
          layer_norm: True

        - input: 'support'
          output: 'support_self'
          module: 'dense'
          repr_dim: 16
          activation: 'tanh'
          layer_norm: True

        - input: 'support_self'
          module: 'self_attn'
          attn_type: 'dot'
          repr_dim: 16
          scaled: True
          num_attn_heads: 8
          key_value_attn: True
          layer_norm: True

        - input: 'support_self'
          module: 'dense'
          activation: 'relu'

        - input: ['support', 'support_self']
          output: 'support'
          module: 'add'

        - input: 'support'
          module: 'dense'
          activation: 'tanh'
          layer_norm: True

  answer_layer:
    support: 'support'
    question: 'question'
    module: 'bilinear'

