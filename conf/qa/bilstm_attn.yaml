escription: >
  BiLSTM with assertions.


name: 'jack_qa_reader'
reader: 'modular_assertion_qa_reader'

assertion_dir: '/data/assertion_store'
assertion_limit: 0
assertion_sources: []
no_reading: True

dropout: &dropout 0.2
repr_dim: 150
max_span_size: 16


model:
  encoder_layer:
  # Shared Contextual Encoding
  - input: 'support'
    output: 'enc_support'
    module: 'conv_glu'
    conv_width: 5
    num_layers: 2
    name: 'contextual_encoding'

  - input: 'question'
    output: 'enc_question'
    module: 'conv_glu'
    conv_width: 5
    num_layers: 2
    name: 'contextual_encoding'  # use same network as support

  - input: ['enc_question', 'question']
    output: 'enc_question'
    module: 'concat'
  - input: ['enc_support', 'support']
    output: 'enc_support'
    module: 'concat'

  # Attention
  - input: 'enc_support'
    dependent: 'enc_question'
    output: 'support'
    module: 'attention_matching'
    attn_type: 'diagonal_bilinear'
    with_sentinel: True  # we gate the attention with an additional scalar sentinel because what we retrieve might actually not be what we were looking for because (softmax) attn always retrieves something
    scaled: True

  - input: 'support'
    module: 'dense'
    activation: 'relu'
    name: 'down_projection'

  # BiLSTM
  - input: 'support'
    module: 'lstm'  # the only application of a RNN
    with_projection: True
    name: 'encoder'
    activation: 'tanh'
    dropout: True

  # Attention
  - input: 'enc_question'
    dependent: 'enc_support'
    output: 'question'
    module: 'attention_matching'
    attn_type: 'diagonal_bilinear'
    with_sentinel: True  # we gate the attention with an additional scalar sentinel because what we retrieve might actually not be what we were looking for because (softmax) attn always retrieves something
    scaled: True

  - input: 'question'
    module: 'dense'
    activation: 'relu'
    name: 'down_projection'

  # BiLSTM Question
  - input: 'question'
    module: 'lstm'  # the only application of a RNN
    with_projection: True
    name: 'encoder'
    activation: 'tanh'
    dropout: True

  answer_layer:
    support: 'support'
    question: 'question'
    module: 'mlp'
